{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "train = pd.read_csv(\"data/linear_train.txt\")\n",
    "# train\n",
    "\n",
    "# encoding=utf8  \n",
    "import sys  \n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "vowels = set(u'аеёиоуыэюя')\n",
    "sign_chars = set(u'ъь')\n",
    "pattern = re.compile(u\"(c*[ьъ]?vc+[ьъ](?=v))|(c*[ьъ]?v(?=v|cv))|(c*[ьъ]?vc[ъь]?(?=cv|ccv))|(c*[ьъ]?v[cьъ]*(?=$))\")\n",
    "\n",
    "def get_syllables(word):\n",
    "    word = word.decode('utf-8').lower()\n",
    "    mask = ''.join(['v' if c in vowels else c if c in sign_chars else 'c' for c in word.lower()])\n",
    "    return ' '.join([word[m.start():m.end()] for m in pattern.finditer(mask)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['syllables'] = pd.Series([get_syllables(word.lower()) for word in list(train.word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['lower'] = [word.decode('utf-8').lower() for word in train.word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(list(train['syllables']), train['class'], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.pipeline import Pipeline, FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>class</th>\n",
       "      <th>syllables</th>\n",
       "      <th>lower</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Аалтонен</td>\n",
       "      <td>1</td>\n",
       "      <td>а ал то нен</td>\n",
       "      <td>аалтонен</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Аар</td>\n",
       "      <td>0</td>\n",
       "      <td>а ар</td>\n",
       "      <td>аар</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Аарон</td>\n",
       "      <td>0</td>\n",
       "      <td>а а рон</td>\n",
       "      <td>аарон</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ААРОН</td>\n",
       "      <td>0</td>\n",
       "      <td>а а рон</td>\n",
       "      <td>аарон</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Аарона</td>\n",
       "      <td>0</td>\n",
       "      <td>а а ро на</td>\n",
       "      <td>аарона</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  class    syllables     lower\n",
       "0  Аалтонен      1  а ал то нен  аалтонен\n",
       "1       Аар      0         а ар       аар\n",
       "2     Аарон      0      а а рон     аарон\n",
       "3     ААРОН      0      а а рон     аарон\n",
       "4    Аарона      0    а а ро на    аарона"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.append(('vs', Pipeline([\n",
    "    ('sa', ItemSelector(key='syllables')),\n",
    "    ('va',\n",
    "        TfidfVectorizer(\n",
    "            min_df=10,\n",
    "            ngram_range=(1, 3),\n",
    "            stop_words=None,\n",
    "            decode_error='replace',\n",
    "            norm='l2',\n",
    "            binary=False,\n",
    "            max_features=5000\n",
    "        ))\n",
    "])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.append(('vc', Pipeline([\n",
    "    ('sb', ItemSelector(key='word')),\n",
    "    ('vb',\n",
    "        TfidfVectorizer(\n",
    "            min_df=5,\n",
    "            ngram_range=(2, 6),\n",
    "            stop_words=None,\n",
    "            decode_error='ignore',\n",
    "            analyzer='char',\n",
    "            norm='l2',\n",
    "            binary=False,\n",
    "            max_features=50000,\n",
    "            smooth_idf = True,\n",
    "            sublinear_tf = True\n",
    "        ))\n",
    "])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_union = FeatureUnion(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create estimator\n",
    "estimators = []\n",
    "estimators.append(('features', feature_union))\n",
    "estimators.append(('logistic', LogisticRegression()))\n",
    "\n",
    "model = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\n",
    "    \"features__vc__vb__max_features\": [100000,200000],\n",
    "    \"features__vc__vb__norm\": ['l2'],\n",
    "    \"features__vc__vb__ngram_range\": [(2,6),(2,4)],\n",
    "    \"logistic__penalty\": ['l1','l2']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, n_jobs=4)\n",
    "start = time()\n",
    "random_search.fit(train, train['class'])\n",
    "\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "report(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 16.31048663,  15.05258036,  14.62201397,  12.47657863,\n",
       "         12.73963245,  13.25814803,  11.88183268,  15.94292092,\n",
       "         12.19800274,  13.60787368,  15.65605267,   9.86104004,\n",
       "         15.88227129,  12.42414904,  16.32648277,  14.29498029,\n",
       "         14.16658131,  12.44642901,  15.75684897,  10.96282101]),\n",
       " 'mean_score_time': array([ 5.48725867,  4.78336827,  4.45470301,  4.60654608,  5.14494395,\n",
       "         4.66697232,  4.24136821,  5.09113169,  4.25181166,  4.57566937,\n",
       "         3.87514408,  4.10048294,  5.05168192,  4.69345665,  5.36444426,\n",
       "         4.08996677,  4.47720059,  4.89074826,  4.72259529,  3.72575633]),\n",
       " 'mean_test_score': array([ 0.88779978,  0.88438782,  0.88884506,  0.88703061,  0.88986076,\n",
       "         0.89168507,  0.89168507,  0.88445685,  0.88708978,  0.88892395,\n",
       "         0.89385453,  0.89269091,  0.88771103,  0.89306564,  0.89303605,\n",
       "         0.88584727,  0.88677422,  0.8889634 ,  0.88670519,  0.88986076]),\n",
       " 'mean_train_score': array([ 0.93100644,  0.92519822,  0.91967597,  0.91709727,  0.9250996 ,\n",
       "         0.91316267,  0.91312816,  0.92520808,  0.93189887,  0.9201    ,\n",
       "         0.93717952,  0.91620484,  0.93098179,  0.93181012,  0.93193831,\n",
       "         0.92926594,  0.93196297,  0.9201    ,  0.93194325,  0.9250996 ]),\n",
       " 'param_features__vc__vb__max_features': masked_array(data = [100000 200000 200000 50000 200000 50000 100000 50000 200000 100000 50000\n",
       "  200000 100000 200000 200000 100000 100000 100000 100000 100000],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_features__vc__vb__min_df': masked_array(data = [20 100 20 5 100 1 5 100 5 100 5 100 20 20 20 1 5 100 5 100],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_features__vc__vb__ngram_range': masked_array(data = [(2, 10) (2, 10) (2, 10) (2, 6) (2, 8) (2, 6) (2, 6) (2, 10) (2, 6) (2, 8)\n",
       "  (2, 6) (2, 6) (2, 10) (2, 6) (2, 10) (2, 6) (2, 10) (2, 8) (2, 10) (2, 8)],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_features__vc__vb__norm': masked_array(data = ['l2' 'l2' 'l1' 'l1' 'l2' 'l1' 'l1' 'l2' 'l2' 'l1' 'l2' 'l1' 'l2' 'l2' 'l2'\n",
       "  'l2' 'l2' 'l1' 'l2' 'l2'],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_features__vc__vb__smooth_idf': masked_array(data = [True True False False False False False False False False False False\n",
       "  False True False False True False True False],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_features__vc__vb__sublinear_tf': masked_array(data = [True True False True True False False False False False False True False\n",
       "  False True True True True False True],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_logistic__penalty': masked_array(data = ['l1' 'l1' 'l1' 'l1' 'l2' 'l2' 'l2' 'l1' 'l1' 'l1' 'l2' 'l2' 'l1' 'l2' 'l2'\n",
       "  'l1' 'l1' 'l1' 'l1' 'l2'],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': ({'features__vc__vb__max_features': 100000,\n",
       "   'features__vc__vb__min_df': 20,\n",
       "   'features__vc__vb__ngram_range': (2, 10),\n",
       "   'features__vc__vb__norm': 'l2',\n",
       "   'features__vc__vb__smooth_idf': True,\n",
       "   'features__vc__vb__sublinear_tf': True,\n",
       "   'logistic__penalty': 'l1'},\n",
       "  {'features__vc__vb__max_features': 200000,\n",
       "   'features__vc__vb__min_df': 100,\n",
       "   'features__vc__vb__ngram_range': (2, 10),\n",
       "   'features__vc__vb__norm': 'l2',\n",
       "   'features__vc__vb__smooth_idf': True,\n",
       "   'features__vc__vb__sublinear_tf': True,\n",
       "   'logistic__penalty': 'l1'},\n",
       "  {'features__vc__vb__max_features': 200000,\n",
       "   'features__vc__vb__min_df': 20,\n",
       "   'features__vc__vb__ngram_range': (2, 10),\n",
       "   'features__vc__vb__norm': 'l1',\n",
       "   'features__vc__vb__smooth_idf': False,\n",
       "   'features__vc__vb__sublinear_tf': False,\n",
       "   'logistic__penalty': 'l1'},\n",
       "  {'features__vc__vb__max_features': 50000,\n",
       "   'features__vc__vb__min_df': 5,\n",
       "   'features__vc__vb__ngram_range': (2, 6),\n",
       "   'features__vc__vb__norm': 'l1',\n",
       "   'features__vc__vb__smooth_idf': False,\n",
       "   'features__vc__vb__sublinear_tf': True,\n",
       "   'logistic__penalty': 'l1'},\n",
       "  {'features__vc__vb__max_features': 200000,\n",
       "   'features__vc__vb__min_df': 100,\n",
       "   'features__vc__vb__ngram_range': (2, 8),\n",
       "   'features__vc__vb__norm': 'l2',\n",
       "   'features__vc__vb__smooth_idf': False,\n",
       "   'features__vc__vb__sublinear_tf': True,\n",
       "   'logistic__penalty': 'l2'},\n",
       "  {'features__vc__vb__max_features': 50000,\n",
       "   'features__vc__vb__min_df': 1,\n",
       "   'features__vc__vb__ngram_range': (2, 6),\n",
       "   'features__vc__vb__norm': 'l1',\n",
       "   'features__vc__vb__smooth_idf': False,\n",
       "   'features__vc__vb__sublinear_tf': False,\n",
       "   'logistic__penalty': 'l2'},\n",
       "  {'features__vc__vb__max_features': 100000,\n",
       "   'features__vc__vb__min_df': 5,\n",
       "   'features__vc__vb__ngram_range': (2, 6),\n",
       "   'features__vc__vb__norm': 'l1',\n",
       "   'features__vc__vb__smooth_idf': False,\n",
       "   'features__vc__vb__sublinear_tf': False,\n",
       "   'logistic__penalty': 'l2'},\n",
       "  {'features__vc__vb__max_features': 50000,\n",
       "   'features__vc__vb__min_df': 100,\n",
       "   'features__vc__vb__ngram_range': (2, 10),\n",
       "   'features__vc__vb__norm': 'l2',\n",
       "   'features__vc__vb__smooth_idf': False,\n",
       "   'features__vc__vb__sublinear_tf': False,\n",
       "   'logistic__penalty': 'l1'},\n",
       "  {'features__vc__vb__max_features': 200000,\n",
       "   'features__vc__vb__min_df': 5,\n",
       "   'features__vc__vb__ngram_range': (2, 6),\n",
       "   'features__vc__vb__norm': 'l2',\n",
       "   'features__vc__vb__smooth_idf': False,\n",
       "   'features__vc__vb__sublinear_tf': False,\n",
       "   'logistic__penalty': 'l1'},\n",
       "  {'features__vc__vb__max_features': 100000,\n",
       "   'features__vc__vb__min_df': 100,\n",
       "   'features__vc__vb__ngram_range': (2, 8),\n",
       "   'features__vc__vb__norm': 'l1',\n",
       "   'features__vc__vb__smooth_idf': False,\n",
       "   'features__vc__vb__sublinear_tf': False,\n",
       "   'logistic__penalty': 'l1'},\n",
       "  {'features__vc__vb__max_features': 50000,\n",
       "   'features__vc__vb__min_df': 5,\n",
       "   'features__vc__vb__ngram_range': (2, 6),\n",
       "   'features__vc__vb__norm': 'l2',\n",
       "   'features__vc__vb__smooth_idf': False,\n",
       "   'features__vc__vb__sublinear_tf': False,\n",
       "   'logistic__penalty': 'l2'},\n",
       "  {'features__vc__vb__max_features': 200000,\n",
       "   'features__vc__vb__min_df': 100,\n",
       "   'features__vc__vb__ngram_range': (2, 6),\n",
       "   'features__vc__vb__norm': 'l1',\n",
       "   'features__vc__vb__smooth_idf': False,\n",
       "   'features__vc__vb__sublinear_tf': True,\n",
       "   'logistic__penalty': 'l2'},\n",
       "  {'features__vc__vb__max_features': 100000,\n",
       "   'features__vc__vb__min_df': 20,\n",
       "   'features__vc__vb__ngram_range': (2, 10),\n",
       "   'features__vc__vb__norm': 'l2',\n",
       "   'features__vc__vb__smooth_idf': False,\n",
       "   'features__vc__vb__sublinear_tf': False,\n",
       "   'logistic__penalty': 'l1'},\n",
       "  {'features__vc__vb__max_features': 200000,\n",
       "   'features__vc__vb__min_df': 20,\n",
       "   'features__vc__vb__ngram_range': (2, 6),\n",
       "   'features__vc__vb__norm': 'l2',\n",
       "   'features__vc__vb__smooth_idf': True,\n",
       "   'features__vc__vb__sublinear_tf': False,\n",
       "   'logistic__penalty': 'l2'},\n",
       "  {'features__vc__vb__max_features': 200000,\n",
       "   'features__vc__vb__min_df': 20,\n",
       "   'features__vc__vb__ngram_range': (2, 10),\n",
       "   'features__vc__vb__norm': 'l2',\n",
       "   'features__vc__vb__smooth_idf': False,\n",
       "   'features__vc__vb__sublinear_tf': True,\n",
       "   'logistic__penalty': 'l2'},\n",
       "  {'features__vc__vb__max_features': 100000,\n",
       "   'features__vc__vb__min_df': 1,\n",
       "   'features__vc__vb__ngram_range': (2, 6),\n",
       "   'features__vc__vb__norm': 'l2',\n",
       "   'features__vc__vb__smooth_idf': False,\n",
       "   'features__vc__vb__sublinear_tf': True,\n",
       "   'logistic__penalty': 'l1'},\n",
       "  {'features__vc__vb__max_features': 100000,\n",
       "   'features__vc__vb__min_df': 5,\n",
       "   'features__vc__vb__ngram_range': (2, 10),\n",
       "   'features__vc__vb__norm': 'l2',\n",
       "   'features__vc__vb__smooth_idf': True,\n",
       "   'features__vc__vb__sublinear_tf': True,\n",
       "   'logistic__penalty': 'l1'},\n",
       "  {'features__vc__vb__max_features': 100000,\n",
       "   'features__vc__vb__min_df': 100,\n",
       "   'features__vc__vb__ngram_range': (2, 8),\n",
       "   'features__vc__vb__norm': 'l1',\n",
       "   'features__vc__vb__smooth_idf': False,\n",
       "   'features__vc__vb__sublinear_tf': True,\n",
       "   'logistic__penalty': 'l1'},\n",
       "  {'features__vc__vb__max_features': 100000,\n",
       "   'features__vc__vb__min_df': 5,\n",
       "   'features__vc__vb__ngram_range': (2, 10),\n",
       "   'features__vc__vb__norm': 'l2',\n",
       "   'features__vc__vb__smooth_idf': True,\n",
       "   'features__vc__vb__sublinear_tf': False,\n",
       "   'logistic__penalty': 'l1'},\n",
       "  {'features__vc__vb__max_features': 100000,\n",
       "   'features__vc__vb__min_df': 100,\n",
       "   'features__vc__vb__ngram_range': (2, 8),\n",
       "   'features__vc__vb__norm': 'l2',\n",
       "   'features__vc__vb__smooth_idf': False,\n",
       "   'features__vc__vb__sublinear_tf': True,\n",
       "   'logistic__penalty': 'l2'}),\n",
       " 'rank_test_score': array([12, 20, 11, 15,  7,  5,  5, 19, 14, 10,  1,  4, 13,  2,  3, 18, 16,\n",
       "         9, 17,  7], dtype=int32),\n",
       " 'split0_test_score': array([ 0.87462651,  0.87110611,  0.88143064,  0.87841316,  0.88054315,\n",
       "         0.88533562,  0.88530604,  0.87107653,  0.87365027,  0.88178564,\n",
       "         0.88492146,  0.88645978,  0.87459693,  0.8836198 ,  0.88356063,\n",
       "         0.87243736,  0.87288111,  0.88172647,  0.87291069,  0.88054315]),\n",
       " 'split0_train_score': array([ 0.93315583,  0.9278012 ,  0.92172177,  0.9189557 ,  0.92749057,\n",
       "         0.91485837,  0.91481399,  0.92800828,  0.93365875,  0.92206198,\n",
       "         0.93867318,  0.91794986,  0.93320021,  0.93342208,  0.93349604,\n",
       "         0.93072998,  0.9337475 ,  0.92206198,  0.9337623 ,  0.92749057]),\n",
       " 'split1_test_score': array([ 0.88474396,  0.88101648,  0.88637103,  0.88557229,  0.88838269,\n",
       "         0.89243558,  0.89246517,  0.88119398,  0.88498062,  0.88533562,\n",
       "         0.89329349,  0.89284975,  0.88459604,  0.89142975,  0.89145934,\n",
       "         0.8839748 ,  0.88515812,  0.88521729,  0.88492146,  0.88838269]),\n",
       " 'split1_train_score': array([ 0.93077435,  0.92468013,  0.91854153,  0.91542046,  0.92414762,\n",
       "         0.91170771,  0.91172251,  0.92450263,  0.93198728,  0.91900007,\n",
       "         0.93657274,  0.91519858,  0.9306856 ,  0.9315879 ,  0.93164707,\n",
       "         0.92864433,  0.93183936,  0.91901487,  0.93189853,  0.92414762]),\n",
       " 'split2_test_score': array([ 0.90402935,  0.90104136,  0.8987338 ,  0.89710668,  0.90065677,\n",
       "         0.89728418,  0.89728418,  0.90110053,  0.9026389 ,  0.89965091,\n",
       "         0.90334891,  0.89876339,  0.9039406 ,  0.90414768,  0.90408852,\n",
       "         0.90113011,  0.90228389,  0.89994675,  0.90228389,  0.90065677]),\n",
       " 'split2_train_score': array([ 0.92908913,  0.92311333,  0.91876461,  0.91691566,  0.92366062,\n",
       "         0.91292193,  0.91284797,  0.92311333,  0.93005059,  0.91923794,\n",
       "         0.93629264,  0.91546608,  0.92905955,  0.93042038,  0.93067183,\n",
       "         0.92842351,  0.93030204,  0.91922315,  0.93016892,  0.92366062]),\n",
       " 'std_fit_time': array([ 0.88224579,  1.40069214,  0.65076027,  0.42466214,  1.0424411 ,\n",
       "         1.01539505,  0.63528838,  0.27309112,  0.59778594,  0.92539808,\n",
       "         1.86794085,  0.85781374,  1.69789835,  0.96261107,  0.70765355,\n",
       "         0.63606547,  0.3768278 ,  0.81745047,  1.38633522,  0.73994066]),\n",
       " 'std_score_time': array([ 0.38034913,  0.38904729,  0.19159669,  0.34517113,  0.74542436,\n",
       "         0.94725326,  0.77525867,  0.40230152,  0.42780179,  1.12834708,\n",
       "         0.04288213,  0.22792506,  0.49478629,  0.45816471,  0.59952781,\n",
       "         0.29575507,  0.12970798,  0.35581069,  0.63099787,  0.48986646]),\n",
       " 'std_test_score': array([ 0.01219656,  0.01245132,  0.00727737,  0.00770093,  0.00827758,\n",
       "         0.00490675,  0.00492106,  0.01247247,  0.01192813,  0.00772221,\n",
       "         0.00753341,  0.00502417,  0.01218029,  0.00845991,  0.00845429,\n",
       "         0.01178833,  0.01205788,  0.00789596,  0.01205768,  0.00827758]),\n",
       " 'std_train_score': array([ 0.00166831,  0.00194856,  0.00144946,  0.00144896,  0.00170232,\n",
       "         0.00129746,  0.00127755,  0.00205967,  0.00147435,  0.00139072,\n",
       "         0.00106235,  0.00123874,  0.00170334,  0.00123548,  0.00117123,\n",
       "         0.00103915,  0.00140932,  0.00138993,  0.00146733,  0.00170232])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(min_df=5, ngram_range=(1, 3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_feature_matrix = count_vectorizer.fit_transform(X_train)\n",
    "sparse_feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_2_words = {\n",
    "    v: k\n",
    "    for k, v in count_vectorizer.vocabulary_.iteritems()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = LogisticRegression()\n",
    "algo.fit(sparse_feature_matrix, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "W = algo.coef_.shape[1]\n",
    "c = 0\n",
    "topic_words = [\n",
    "    num_2_words[w_num]\n",
    "    for w_num in heapq.nlargest(20, range(W), key=lambda w: algo.coef_[c, w])\n",
    "]\n",
    "print ',  '.join(topic_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = LogisticRegression()\n",
    "arr = cross_val_score(algo, sparse_feature_matrix, y_train, cv=5, scoring='accuracy')\n",
    "print arr\n",
    "print np.mean(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo.fit(sparse_feature_matrix, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(algo.predict(sparse_feature_matrix), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(algo.predict(count_vectorizer.transform(X_test)), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предсказание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/linear_test.txt\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['syllables'] = pd.Series([get_syllables(word.lower()) for word in list(test.word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_feature_matrix = count_vectorizer.fit_transform(train['syllables'])\n",
    "sparse_feature_matrix\n",
    "\n",
    "algo = LogisticRegression(penalty='l1', C=0.2)\n",
    "algo.fit(sparse_feature_matrix, train['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = pd.DataFrame()   \n",
    "solution['Answer'] = pd.Series(algo.predict_proba(count_vectorizer.transform(test['syllables']))[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "solution.to_csv('solution5.txt')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
